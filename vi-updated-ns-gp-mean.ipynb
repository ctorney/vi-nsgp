{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install pickle5","metadata":{"execution":{"iopub.status.busy":"2022-03-09T08:43:46.725524Z","iopub.execute_input":"2022-03-09T08:43:46.725937Z","iopub.status.idle":"2022-03-09T08:43:54.363601Z","shell.execute_reply.started":"2022-03-09T08:43:46.725843Z","shell.execute_reply":"2022-03-09T08:43:54.362776Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport time\n\nimport random\n\nimport matplotlib\nfrom matplotlib import pyplot as plt\n\nimport tensorflow as tf\nimport tensorflow_probability as tfp\nfrom tqdm import tqdm\n\nimport pickle5 as pickle\nfrom scipy import stats\n\nimport matplotlib.ticker as tick\n\nimport sys\nsys.path.append('..')\n#from nsgp_vi import nsgpVI\n\n# We'll use double precision throughout for better numerics.\ndtype = np.float64\n\ntfb = tfp.bijectors\ntfd = tfp.distributions\ntfk = tfp.math.psd_kernels\n\nplt.style.use('ggplot') \nplt.style.use('seaborn-paper')\nplt.style.use('seaborn-whitegrid')\n","metadata":{"execution":{"iopub.status.busy":"2022-03-09T08:43:54.369501Z","iopub.execute_input":"2022-03-09T08:43:54.370007Z","iopub.status.idle":"2022-03-09T08:43:56.485868Z","shell.execute_reply.started":"2022-03-09T08:43:54.369972Z","shell.execute_reply":"2022-03-09T08:43:56.485125Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Extracted from https://github.com/OpenNMT/OpenNMT-tf/blob/master/opennmt/optimizers/utils.py\n\nimport tensorflow as tf\n\nclass GradientAccumulator(object):\n    \"\"\"Gradient accumulation utility.\n    When used with a distribution strategy, the accumulator should be called in a\n    replica context. Gradients will be accumulated locally on each replica and\n    without synchronization. Users should then call ``.gradients``, scale the\n    gradients if required, and pass the result to ``apply_gradients``.\n    \"\"\"\n\n    # We use the ON_READ synchronization policy so that no synchronization is\n    # performed on assignment. To get the value, we call .value() which returns the\n    # value on the current replica without synchronization.\n\n    def __init__(self):\n        \"\"\"Initializes the accumulator.\"\"\"\n        self._gradients = []\n        self._accum_steps = None\n\n    @property\n    def step(self):\n        \"\"\"Number of accumulated steps.\"\"\"\n        if self._accum_steps is None:\n            self._accum_steps = tf.Variable(\n                tf.constant(0, dtype=tf.int64),\n                trainable=False,\n                synchronization=tf.VariableSynchronization.ON_READ,\n                aggregation=tf.VariableAggregation.ONLY_FIRST_REPLICA,\n            )\n        return self._accum_steps.value()\n\n    @property\n    def gradients(self):\n        \"\"\"The accumulated gradients on the current replica.\"\"\"\n        if not self._gradients:\n            raise ValueError(\n                \"The accumulator should be called first to initialize the gradients\"\n            )\n        #return list(gradient.value()/tf.cast(self._accum_steps, gradient.dtype) for gradient in self._gradients)\n        grads = list(gradient.value()/tf.cast(self._accum_steps, gradient.dtype) for gradient in self._gradients)\n        clipped_grads, gn = tf.clip_by_global_norm(grads,clip_norm=1.0)\n        return clipped_grads\n        \n    \n    \n\n    def __call__(self, gradients):\n        \"\"\"Accumulates :obj:`gradients` on the current replica.\"\"\"\n        if not self._gradients:\n            _ = self.step  # Create the step variable.\n            self._gradients.extend(\n                [\n                    tf.Variable(\n                        tf.zeros_like(gradient),\n                        trainable=False,\n                        synchronization=tf.VariableSynchronization.ON_READ,\n                    )\n                    for gradient in gradients\n                ]\n            )\n        if len(gradients) != len(self._gradients):\n            raise ValueError(\n                \"Expected %s gradients, but got %d\"\n                % (len(self._gradients), len(gradients))\n            )\n\n        for accum_gradient, gradient in zip(self._gradients, gradients):\n            accum_gradient.assign_add(gradient, read_value=False)\n        self._accum_steps.assign_add(1)\n\n    def reset(self):\n        \"\"\"Resets the accumulated gradients on the current replica.\"\"\"\n        if not self._gradients:\n            return\n        self._accum_steps.assign(0)\n        for gradient in self._gradients:\n            gradient.assign(\n                tf.zeros(gradient.shape, dtype=gradient.dtype), read_value=False\n            )","metadata":{"execution":{"iopub.status.busy":"2022-03-09T08:43:56.487348Z","iopub.execute_input":"2022-03-09T08:43:56.487619Z","iopub.status.idle":"2022-03-09T08:43:56.502893Z","shell.execute_reply.started":"2022-03-09T08:43:56.487581Z","shell.execute_reply":"2022-03-09T08:43:56.501079Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"\nCopyright 2021 Colin Torney\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n    http://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n\"\"\"\n\nimport tensorflow as tf\nimport tensorflow_probability as tfp\nimport numpy as np\nfrom tqdm import tqdm\n\nfrom tensorflow_probability.python.distributions import kullback_leibler\n\n#from utils.gradient_accumulator import GradientAccumulator\n\ntfb = tfp.bijectors\ntfd = tfp.distributions\ntfk = tfp.math.psd_kernels\n\ndtype = np.float64\n# changed here to 3 instead of 2\nNUM_LATENT = 3\n\nclass nsgpVI(tf.Module):\n                                        \n    def __init__(self,kernel_len,kernel_amp,kernel_mean,n_inducing_points,inducing_index_points,dataset,num_training_points, init_observation_noise_variance=1e-2,num_sequential_samples=10,num_parallel_samples=10,jitter=1e-6):\n               \n        self.jitter=jitter\n        \n        # define the means for all parameters including the mean\n        self.mean_len = tf.Variable([0.0], dtype=tf.float64, name='len_mean', trainable=True)\n        self.mean_amp = tf.Variable([0.0], dtype=tf.float64, name='var_mean', trainable=True)\n        self.mean_mean = tf.Variable([0.0], dtype=tf.float64, name='mean_mean', trainable=True)\n\n        # inducing points for all parameters including the mean, kept fixed\n        self.amp_inducing_index_points = tf.Variable(inducing_index_points,dtype=dtype,name='amp_ind_points',trainable=False) #z's for amplitude\n        self.len_inducing_index_points = tf.Variable(inducing_index_points,dtype=dtype,name='len_ind_points',trainable=False) #z's for len\n        self.mean_inducing_index_points = tf.Variable(inducing_index_points,dtype=dtype,name='mean_ind_points',trainable=False) #z's for mean\n\n        # prior distributions for all parameters including the mean\n        self.kernel_len = kernel_len\n        self.kernel_amp = kernel_amp\n        self.kernel_mean = kernel_mean\n\n        \n        #parameters for variational distribution for len,phi(l_z) and var,phi(sigma_z)\n        self.variational_inducing_observations_loc = tf.Variable(np.zeros((NUM_LATENT*n_inducing_points),dtype=dtype),name='ind_loc_post')\n        self.variational_inducing_observations_scale = tfp.util.TransformedVariable(np.eye(NUM_LATENT*n_inducing_points, dtype=dtype),tfp.bijectors.FillScaleTriL(diag_shift=np.float64(1e-05)),dtype=tf.float64, name='ind_scale_post', trainable=True)\n\n        \n        #approximation to the posterior: phi(l_z)\n        self.variational_inducing_observations_posterior = tfd.MultivariateNormalLinearOperator(\n                                                                      loc=self.variational_inducing_observations_loc,\n                                                                      scale=tf.linalg.LinearOperatorLowerTriangular(self.variational_inducing_observations_scale))\n\n        #p(l_z)\n        self.inducing_prior = tfd.MultivariateNormalDiag(loc=tf.zeros((NUM_LATENT*n_inducing_points),dtype=tf.float64),name='ind_prior')\n        \n        self.vgp_observation_noise_variance = tf.Variable(np.log(np.exp(init_observation_noise_variance)-1),dtype=dtype,name='nv', trainable=False)\n\n        self.num_sequential_samples=num_sequential_samples\n        self.num_parallel_samples=num_parallel_samples\n        \n        self.dataset = dataset\n        self.num_training_points=num_training_points\n        \n\n    def optimize(self, BATCH_SIZE, SEG_LENGTH, NUM_EPOCHS=100):\n\n        strategy = tf.distribute.MirroredStrategy()\n        dist_dataset = strategy.experimental_distribute_dataset(self.dataset)\n\n        initial_learning_rate = 1e-1\n        steps_per_epoch = self.num_training_points//(BATCH_SIZE*SEG_LENGTH)\n        learning_rate = tf.optimizers.schedules.ExponentialDecay(initial_learning_rate=initial_learning_rate,decay_steps=steps_per_epoch,decay_rate=0.99,staircase=True)\n        optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n        accumulator = GradientAccumulator()\n\n        def train_step(inputs):\n            x_train_batch, y_train_batch = inputs\n            kl_weight = tf.reduce_sum(tf.ones_like(x_train_batch))/self.num_training_points\n\n            with tf.GradientTape(watch_accessed_variables=True) as tape:\n                loss = self.variational_loss(observations=y_train_batch,observation_index_points=x_train_batch,kl_weight=kl_weight) \n            grads = tape.gradient(loss, self.trainable_variables)\n            return loss, grads\n\n        @tf.function\n        def distributed_train_step(dataset_inputs):\n            per_replica_losses, per_replica_grads = strategy.run(train_step, args=(dataset_inputs,))\n            return strategy.reduce(tf.distribute.ReduceOp.SUM, per_replica_losses, axis=None), strategy.reduce(tf.distribute.ReduceOp.SUM, per_replica_grads, axis=None)\n            #return strategy.reduce(tf.distribute.ReduceOp.SUM, per_replica_losses, axis=None), [strategy.reduce(tf.distribute.ReduceOp.SUM, prg, axis=None) for prg in per_replica_grads]\n\n        pbar = tqdm(range(NUM_EPOCHS))\n        loss_history = np.zeros((NUM_EPOCHS))\n\n        for i in pbar:\n            batch_count=0    \n            epoch_loss = 0.0\n            for batch in self.dataset:\n                batch_loss = 0.0\n                for s in range(self.num_sequential_samples):\n                    loss, grads = distributed_train_step(batch)\n                    # accumulate the loss and gradient\n                    accumulator(grads)\n                    batch_loss += loss.numpy()\n                grads = accumulator.gradients\n                optimizer.apply_gradients(zip(grads, self.trainable_variables))\n                accumulator.reset()\n                batch_loss/=self.num_sequential_samples\n                epoch_loss+=batch_loss\n                batch_count+=1\n                pbar.set_description(\"Loss %f, klen %f\" % (epoch_loss/batch_count, self.kernel_len.length_scale.numpy()))\n            loss_history[i] = epoch_loss/batch_count\n\n        return loss_history\n\n\n    def variational_loss(self,observations,observation_index_points,kl_weight=1.0):\n        \n        kl_penalty = self.surrogate_posterior_kl_divergence_prior()\n        recon = self.surrogate_posterior_expected_log_likelihood(observations,observation_index_points)\n        return (-recon + kl_weight*kl_penalty)\n\n    \n    def surrogate_posterior_kl_divergence_prior(self):\n        return kullback_leibler.kl_divergence(self.variational_inducing_observations_posterior,self.inducing_prior) \n\n    \n    def surrogate_posterior_expected_log_likelihood(self,observations,observation_index_points):\n\n        #added mean_vals \n        mean_vals,len_vals, amp_vals = self.get_samples(observation_index_points,S=self.num_parallel_samples) \n        K = self.non_stat_matern12(observation_index_points, len_vals, amp_vals) # BxNxN\n        K = K + (tf.eye(tf.shape(K)[-1], dtype=tf.float64) * tf.nn.softplus(self.vgp_observation_noise_variance))\n\n        #added a mean, watch the shape here !\n        logpdf = tf.reduce_sum(tf.reduce_mean(tfd.MultivariateNormalTriL(loc = mean_vals[...,0],scale_tril = tf.linalg.cholesky(K)).log_prob((observations[...,0])),axis=0))\n\n        return logpdf\n    \n    def get_samples(self,observation_index_points,S=1):\n        mean, var = self.get_conditional(observation_index_points)\n        samples = self.sample_conditional(mean, var, S)\n    \n        # added a mean on this line and the line below\n        mean_samples,len_samples,amp_samples = tf.split(samples,NUM_LATENT,axis=2)\n        \n        return tf.math.softplus(self.mean_mean + mean_samples),tf.math.softplus(self.mean_len + len_samples), tf.math.softplus(self.mean_amp + amp_samples)\n    \n    def get_conditional(self, observation_index_points):\n        \n        Xnew = observation_index_points\n\n        # added Z_mean\n        Z_amp = self.amp_inducing_index_points \n        Z_len = self.len_inducing_index_points \n        Z_mean = self.mean_inducing_index_points \n        \n        # added a kernel_mean\n        kernel_amp = self.kernel_amp\n        kernel_len = self.kernel_len\n        kernel_mean = self.kernel_mean\n\n        f = self.variational_inducing_observations_loc\n        q_sqrt = self.variational_inducing_observations_scale\n        M = tf.shape(f)[0]\n        \n        # add Kmm_mean\n        Kmm_amp = tf.linalg.LinearOperatorFullMatrix(kernel_amp.matrix(Z_amp,Z_amp) + self.jitter * tf.eye(M//3, dtype=tf.float64),is_positive_definite=True,is_self_adjoint=True)\n        Kmm_len = tf.linalg.LinearOperatorFullMatrix(kernel_len.matrix(Z_len,Z_len) + self.jitter * tf.eye(M//3, dtype=tf.float64),is_positive_definite=True,is_self_adjoint=True)        \n        Kmm_mean = tf.linalg.LinearOperatorFullMatrix(kernel_mean.matrix(Z_mean,Z_mean) + self.jitter * tf.eye(M//3, dtype=tf.float64),is_positive_definite=True,is_self_adjoint=True)\n\n        # added Kmm_mean\n        Kmm = tf.linalg.LinearOperatorBlockDiag([Kmm_mean,Kmm_len,Kmm_amp])\n\n        # added Kmn_mean\n        Kmn_amp = tf.linalg.LinearOperatorFullMatrix(kernel_amp.matrix(Z_amp, Xnew),is_positive_definite=True,is_self_adjoint=True)\n        Kmn_len = tf.linalg.LinearOperatorFullMatrix(kernel_len.matrix(Z_len, Xnew),is_positive_definite=True,is_self_adjoint=True)        \n        Kmn_mean = tf.linalg.LinearOperatorFullMatrix(kernel_mean.matrix(Z_mean, Xnew),is_positive_definite=True,is_self_adjoint=True)\n\n        #added Kmn_mean\n        Kmn = tf.linalg.LinearOperatorBlockDiag([Kmn_mean,Kmn_len,Kmn_amp])\n\n        #added Knn_mean\n        Knn_amp = tf.linalg.LinearOperatorFullMatrix(kernel_amp.matrix(Xnew, Xnew),is_positive_definite=True,is_self_adjoint=True)\n        Knn_len = tf.linalg.LinearOperatorFullMatrix(kernel_len.matrix(Xnew, Xnew),is_positive_definite=True,is_self_adjoint=True)\n        Knn_mean = tf.linalg.LinearOperatorFullMatrix(kernel_mean.matrix(Xnew, Xnew),is_positive_definite=True,is_self_adjoint=True)\n\n        #added Knn_mean\n        Knn = tf.linalg.LinearOperatorBlockDiag([Knn_mean,Knn_len,Knn_amp])\n\n        mean,var = self.full_conditional_lo(Kmn,Kmm,Knn,f,q_sqrt)\n        \n        return mean, var\n\n    def get_marginal(self, observation_index_points):\n        \n        Xnew = observation_index_points\n\n        # added Knn_mean\n        Z_amp = self.amp_inducing_index_points \n        Z_len = self.len_inducing_index_points \n        Z_mean = self.mean_inducing_index_points \n\n\n        # added kernel_mean\n        kernel_amp = self.kernel_amp\n        kernel_len = self.kernel_len\n        kernel_mean = self.kernel_mean\n\n\n        f = self.variational_inducing_observations_loc\n        q_sqrt = self.variational_inducing_observations_scale\n\n\n        \n        M = tf.shape(f)[0]\n        \n        #added Kmm_mean\n        Kmm_amp = tf.linalg.LinearOperatorFullMatrix(kernel_amp.matrix(Z_amp,Z_amp) + self.jitter * tf.eye(M//3, dtype=tf.float64),is_positive_definite=True,is_self_adjoint=True)\n        Kmm_len = tf.linalg.LinearOperatorFullMatrix(kernel_len.matrix(Z_len,Z_len) + self.jitter * tf.eye(M//3, dtype=tf.float64),is_positive_definite=True,is_self_adjoint=True)\n        Kmm_mean = tf.linalg.LinearOperatorFullMatrix(kernel_mean.matrix(Z_mean,Z_mean) + self.jitter * tf.eye(M//3, dtype=tf.float64),is_positive_definite=True,is_self_adjoint=True)\n\n\n        \n        Kmm = tf.linalg.LinearOperatorBlockDiag([Kmm_mean,Kmm_len,Kmm_amp])\n\n        # added Kmn_mean\n        Kmn_amp = tf.linalg.LinearOperatorFullMatrix(kernel_amp.matrix(Z_amp, Xnew),is_positive_definite=True,is_self_adjoint=True)\n        Kmn_len = tf.linalg.LinearOperatorFullMatrix(kernel_len.matrix(Z_len, Xnew),is_positive_definite=True,is_self_adjoint=True)\n        Kmn_mean = tf.linalg.LinearOperatorFullMatrix(kernel_mean.matrix(Z_mean, Xnew),is_positive_definite=True,is_self_adjoint=True)\n\n\n        # added Kmn_mean\n        Kmn = tf.linalg.LinearOperatorBlockDiag([Kmn_mean,Kmn_len,Kmn_amp])\n\n        #added Knn_mean\n        Knn_amp = tf.linalg.LinearOperatorFullMatrix(kernel_amp.matrix(Xnew, Xnew),is_positive_definite=True,is_self_adjoint=True)\n        Knn_len = tf.linalg.LinearOperatorFullMatrix(kernel_len.matrix(Xnew, Xnew),is_positive_definite=True,is_self_adjoint=True)\n        Knn_mean = tf.linalg.LinearOperatorFullMatrix(kernel_mean.matrix(Xnew, Xnew),is_positive_definite=True,is_self_adjoint=True)\n\n        Knn = tf.linalg.LinearOperatorBlockDiag([Knn_mean,Knn_len,Knn_amp])\n\n        mean,var = self.marginal(Kmn.to_dense(),Kmm.to_dense(),Knn.to_dense(),f,q_sqrt)\n        \n        mean_list = tf.split(mean,NUM_LATENT,axis=1)\n        var_list = tf.split(var,NUM_LATENT,axis=0)\n\n        return mean_list, var_list\n        \n\n    def sample_conditional(self, mean, var, S=1):\n        # mean BxNx1\n        # var BxNxN\n        # returns SxBxNx1\n        B = tf.shape(mean)[0]\n        N = tf.shape(mean)[1]\n        z = tf.random.normal((S,B,N,1),dtype=tf.float64)\n        \n        I = self.jitter * tf.eye(N, dtype=tf.float64) #NN\n        chol = tf.linalg.cholesky(var + I)  # BNN\n        samples = mean + tf.matmul(chol, z)#[:, :, :, 0]  # BSN1\n\n        return samples\n\n    def full_conditional(self, Kmn, Kmm, Knn, f, q_sqrt, full_cov=True):\n\n        f = tf.expand_dims(f,-1)\n        q_sqrt= tf.expand_dims(q_sqrt,0)\n\n        Lm = tf.linalg.cholesky(Kmm)\n\n        N = tf.shape(Kmn)[-1]\n        M = tf.shape(f)[0]\n\n        # Compute the projection matrix A\n        Lm = tf.broadcast_to(Lm, tf.shape(Lm))\n        A = tf.linalg.triangular_solve(Lm, Kmn, lower=True)  # [..., M, N]\n\n        # compute the covariance due to the conditioning\n        fvar = Knn - tf.linalg.matmul(A, A, transpose_a=True)  # [..., N, N]\n\n        # construct the conditional mean\n        f_shape = [M, 1]\n        f = tf.broadcast_to(f, f_shape)  # [..., M, R]\n        fmean = tf.linalg.matmul(A, f, transpose_a=True)  # [..., N, R]\n\n        L = tf.linalg.band_part(q_sqrt, -1, 0)  \n\n        LTA = tf.linalg.matmul(L, A, transpose_a=True)  # [R, M, N]\n\n        fvar = fvar + tf.linalg.matmul(LTA, LTA, transpose_a=True)  # [R, N, N]\n\n        return fmean, fvar\n    \n    def full_conditional_lo(self, Kmn, Kmm, Knn, f, q_sqrt, full_cov=True):\n\n        f = tf.expand_dims(f,0)\n        q_sqrt= tf.expand_dims(q_sqrt,0)\n\n        Lm  = Kmm.cholesky()\n        A = Lm.solve(Kmn)\n        \n        fmean = A.matvec(f,adjoint=True)\n\n        B = A.matmul(A,adjoint=True)\n\n\n        L = tf.linalg.LinearOperatorLowerTriangular(q_sqrt)\n\n        LTA = L.matmul(A,adjoint=True)\n\n        LTA = LTA.matmul(LTA,adjoint=True)\n\n        fvar = Knn.to_dense() - B.to_dense() + LTA.to_dense()\n        \n        return tf.expand_dims(fmean,-1), fvar\n\n    def marginal(self, Kmn, Kmm, Knn, f, q_sqrt, full_cov=True):\n\n        f = tf.expand_dims(f,-1)\n        q_sqrt= tf.expand_dims(q_sqrt,0)\n\n        Knn = tf.linalg.diag_part(Knn)\n        Lm = tf.linalg.cholesky(Kmm)\n\n        N = tf.shape(Kmn)[-1]\n        M = tf.shape(f)[0]\n\n        # Compute the projection matrix A\n        Lm = tf.broadcast_to(Lm, tf.shape(Lm))\n        A = tf.linalg.triangular_solve(Lm, Kmn, lower=True)  # [..., M, N]\n\n        # compute the covariance due to the conditioning\n        fvar = Knn - tf.reduce_sum(tf.square(A), -2)  # [..., N]\n\n        # construct the conditional mean\n        f_shape = [M, 1]\n        f = tf.broadcast_to(f, f_shape)  # [..., M, R]\n        fmean = tf.linalg.matmul(A, f, transpose_a=True)  # [..., N, R]\n\n        L = tf.linalg.band_part(q_sqrt, -1, 0)  \n\n        LTA = tf.linalg.matmul(L, A, transpose_a=True)  # [R, M, N]\n\n        fvar = fvar + tf.reduce_sum(tf.square(LTA), -2)  # [R, N]\n        fvar = tf.linalg.adjoint(fvar)  # [N, R]\n\n        return fmean, fvar\n    \n        \n    def non_stat_matern12(self, X, lengthscales, stddev):\n        ''' Non-stationary Matern 12 kernel'''\n        \n        Xs = tf.reduce_sum(input_tensor=tf.square(X), axis=-1, keepdims=True)#(1000,1)\n        Ls = tf.square(lengthscales)#(1,1000,1)\n        \n        dist = -2 * tf.matmul(X, X, transpose_b=True)\n        dist += Xs + tf.linalg.matrix_transpose(Xs)\n        Lscale = Ls + tf.linalg.matrix_transpose(Ls)\n        dist = tf.divide(2*dist,Lscale)\n        dist = tf.sqrt(tf.maximum(dist, 1e-40))\n        prefactL = 2 * tf.matmul(lengthscales, lengthscales, transpose_b=True)\n        prefactV = tf.matmul(stddev, stddev, transpose_b=True)\n\n        return tf.multiply(prefactV,tf.multiply( tf.sqrt(tf.maximum(tf.divide(prefactL,Lscale), 1e-40)),tf.exp(-dist)))\n\n\n    def non_stat_vel(self,T,lengthscales, stddev):\n        \n        \"\"\"Non-stationary integrated Matern12 kernel\"\"\"\n\n        sigma_ = 0.5*(stddev[...,:-1,0,None] + stddev[...,1:,0,None])\n        len_ = 0.5*(lengthscales[...,:-1,0,None] + lengthscales[...,1:,0,None])\n\n        Ls = tf.square(len_)\n\n        L = tf.math.sqrt(0.5*(Ls + tf.linalg.matrix_transpose(Ls)))\n\n        prefactL = tf.math.sqrt(tf.matmul(len_, len_, transpose_b=True))\n        prefactV = tf.matmul(sigma_, sigma_,transpose_b=True)\n\n        zeta = tf.math.multiply(prefactV,tf.math.divide(prefactL,L))\n    \n\n        tpq1 = tf.math.exp(tf.math.divide(-tf.math.abs(tf.linalg.matrix_transpose(T[:-1]) - T[1:]),L))\n        tp1q1 = tf.math.exp(tf.math.divide(-tf.math.abs(tf.linalg.matrix_transpose(T[1:]) - T[1:]),L))\n        tpq = tf.math.exp(tf.math.divide(-tf.math.abs(tf.linalg.matrix_transpose(T[:-1]) - T[:-1]),L))\n        tp1q = tf.math.exp(tf.math.divide(-tf.math.abs(tf.linalg.matrix_transpose(T[1:]) - T[:-1]),L))\n\n\n        Epq_grid = tpq1-tp1q1-tpq+tp1q\n        Epq_grid = (L**2)*Epq_grid\n                \n        Epq_grid = tf.linalg.set_diag(Epq_grid,(tf.linalg.diag_part(Epq_grid)) + 2.0*tf.squeeze(len_)[:]*(tf.squeeze(T[1:])-tf.squeeze(T[:-1])))\n        Epq_grid = zeta*Epq_grid\n        \n        \n        K = tf.math.cumsum(tf.math.cumsum(Epq_grid,axis=-2,exclusive=False),axis=-1,exclusive=False)\n        \n        return K","metadata":{"execution":{"iopub.status.busy":"2022-03-09T08:55:05.536621Z","iopub.execute_input":"2022-03-09T08:55:05.536901Z","iopub.status.idle":"2022-03-09T08:55:05.615639Z","shell.execute_reply.started":"2022-03-09T08:55:05.536869Z","shell.execute_reply":"2022-03-09T08:55:05.614827Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(tfp.__version__)\nprint(tf.__version__)","metadata":{"tags":[],"execution":{"iopub.status.busy":"2022-03-09T08:46:05.666372Z","iopub.execute_input":"2022-03-09T08:46:05.667001Z","iopub.status.idle":"2022-03-09T08:46:05.672192Z","shell.execute_reply.started":"2022-03-09T08:46:05.66696Z","shell.execute_reply":"2022-03-09T08:46:05.671157Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv('../input/realvidata/Tetuan City power consumption.csv', thousands=',')\n\ndts = pd.DatetimeIndex(df['DateTime'],dayfirst=True)\nT = (24*(dts.day-1) + dts.hour  +dts.minute/60.).astype(float).values[:,None]\n# in hours !!!\nX = df['Zone 1 Power Consumption'].astype('float').values.reshape(len(T),1)\n# 3 zones\n\nX = np.log(X)\n#meanX = np.mean(X)\n#X = X - meanX\n#X = X/X.std()\n\nprint(T.shape)\nprint(X.shape)\n","metadata":{"tags":[],"execution":{"iopub.status.busy":"2022-03-09T09:01:06.613446Z","iopub.execute_input":"2022-03-09T09:01:06.614184Z","iopub.status.idle":"2022-03-09T09:01:11.105461Z","shell.execute_reply.started":"2022-03-09T09:01:06.614132Z","shell.execute_reply":"2022-03-09T09:01:11.104617Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df","metadata":{"execution":{"iopub.status.busy":"2022-03-09T09:01:17.376259Z","iopub.execute_input":"2022-03-09T09:01:17.376521Z","iopub.status.idle":"2022-03-09T09:01:17.396807Z","shell.execute_reply.started":"2022-03-09T09:01:17.376493Z","shell.execute_reply":"2022-03-09T09:01:17.395891Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_training_points_ = T.shape[0]\n\nnum_inducing_points_ = 48\n\ninducing_index_points = np.linspace(0., 24., num_inducing_points_, endpoint=False)[..., np.newaxis]\nnp.random.shuffle(inducing_index_points)\n","metadata":{"execution":{"iopub.status.busy":"2022-03-09T09:01:26.75669Z","iopub.execute_input":"2022-03-09T09:01:26.756885Z","iopub.status.idle":"2022-03-09T09:01:26.762323Z","shell.execute_reply.started":"2022-03-09T09:01:26.756861Z","shell.execute_reply":"2022-03-09T09:01:26.761546Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"BATCH_SIZE=8\nSEG_LENGTH=500 #1024\n\n\nclass segment_generator:\n    def __iter__(self):\n        \n        # loop over segments\n        self.j = 0\n        self.max_j = num_training_points_//SEG_LENGTH\n        \n        \n        return self\n\n    def __next__(self):\n        \n\n        if self.j==self.max_j:\n            raise StopIteration\n\n        TT = T[self.j*SEG_LENGTH:(self.j+1)*SEG_LENGTH]\n        XX = X[self.j*SEG_LENGTH:(self.j+1)*SEG_LENGTH]\n    \n        self.j += 1\n\n        return TT,XX\n\n        \ndataset = tf.data.Dataset.from_generator(segment_generator, (tf.float64)) \ndataset = dataset.map(lambda dd: (dd[0],dd[1]))\ndataset = dataset.shuffle(1000)\ndataset = dataset.batch(BATCH_SIZE, drop_remainder=True)","metadata":{"execution":{"iopub.status.busy":"2022-03-09T09:05:27.44914Z","iopub.execute_input":"2022-03-09T09:05:27.449928Z","iopub.status.idle":"2022-03-09T09:05:27.496423Z","shell.execute_reply.started":"2022-03-09T09:05:27.449881Z","shell.execute_reply":"2022-03-09T09:05:27.495748Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset = tf.data.Dataset.from_generator(segment_generator, (tf.float64)) \ndataset = dataset.map(lambda dd: (dd[0],dd[1]))\ndataset = dataset.shuffle(1000)\ndataset = dataset.batch(BATCH_SIZE)","metadata":{"execution":{"iopub.status.busy":"2022-03-09T09:05:28.210745Z","iopub.execute_input":"2022-03-09T09:05:28.211436Z","iopub.status.idle":"2022-03-09T09:05:28.240866Z","shell.execute_reply.started":"2022-03-09T09:05:28.211397Z","shell.execute_reply":"2022-03-09T09:05:28.240205Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for d in dataset:\n    print(d[0].shape,d[1].shape)","metadata":{"execution":{"iopub.status.busy":"2022-03-09T09:05:28.641266Z","iopub.execute_input":"2022-03-09T09:05:28.641879Z","iopub.status.idle":"2022-03-09T09:05:28.892002Z","shell.execute_reply.started":"2022-03-09T09:05:28.641843Z","shell.execute_reply":"2022-03-09T09:05:28.891254Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#lengthscale kernel parameters,lower levels\nkernel_len_a = tfp.util.TransformedVariable(1.0, tfb.Softplus(),dtype=tf.float64, name='k_len_a',trainable=True)\nkernel_len_l = tfp.util.TransformedVariable(1.0, tfb.Chain([tfb.Shift(np.float64(0.5)), tfb.Softplus()]),dtype=tf.float64, name='k_len_l',trainable=True)\n\n# amplitude kernel parameters, lower levels\nkernel_amp_a = tfp.util.TransformedVariable(1.0, tfb.Softplus(),dtype=tf.float64, name='k_amp_a',trainable=True)\nkernel_amp_l = tfp.util.TransformedVariable(1.0, tfb.Chain([tfb.Shift(np.float64(0.5)), tfb.Softplus()]),dtype=tf.float64, name='k_amp_l',trainable=True)\n\n# mean kernel parameters, lower levels\nkernel_mean_a = tfp.util.TransformedVariable(1.0, tfb.Softplus(),dtype=tf.float64, name='k_mean_a',trainable=True)\nkernel_mean_l = tfp.util.TransformedVariable(1.0, tfb.Chain([tfb.Shift(np.float64(0.5)), tfb.Softplus()]),dtype=tf.float64, name='k_mean_l',trainable=True)\n\n#kernels on the second layer\nkernel_len = tfk.ExpSinSquared(kernel_len_a,kernel_len_l,period=np.float64(24.0))\nkernel_amp = tfk.ExpSinSquared(kernel_amp_a,kernel_amp_l,period=np.float64(24.0))\nkernel_mean = tfk.ExpSinSquared(kernel_mean_a,kernel_mean_l,period=np.float64(24.0))\n\n#added kernel_mean\nvgp = nsgpVI(kernel_len,kernel_amp,kernel_mean,n_inducing_points=num_inducing_points_,inducing_index_points=inducing_index_points,dataset=dataset,num_training_points=num_training_points_, num_sequential_samples=20,num_parallel_samples=10,init_observation_noise_variance=0.005**2)  \n","metadata":{"execution":{"iopub.status.busy":"2022-03-09T09:05:31.119669Z","iopub.execute_input":"2022-03-09T09:05:31.120274Z","iopub.status.idle":"2022-03-09T09:05:31.174137Z","shell.execute_reply.started":"2022-03-09T09:05:31.120233Z","shell.execute_reply":"2022-03-09T09:05:31.173458Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#vgp.trainable_variables","metadata":{"execution":{"iopub.status.busy":"2022-03-09T09:05:33.038906Z","iopub.execute_input":"2022-03-09T09:05:33.039606Z","iopub.status.idle":"2022-03-09T09:05:33.04524Z","shell.execute_reply.started":"2022-03-09T09:05:33.039566Z","shell.execute_reply":"2022-03-09T09:05:33.044557Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"loss = vgp.optimize(BATCH_SIZE, SEG_LENGTH, NUM_EPOCHS=500)\n","metadata":{"execution":{"iopub.status.busy":"2022-03-09T09:05:33.392452Z","iopub.execute_input":"2022-03-09T09:05:33.392707Z","iopub.status.idle":"2022-03-09T09:26:19.307599Z","shell.execute_reply.started":"2022-03-09T09:05:33.392679Z","shell.execute_reply":"2022-03-09T09:26:19.304814Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#ZZ = np.linspace(0,24*60,200)[:,None]\nZZ = np.linspace(0,24,200)[:,None]\n\n[mean_mean,len_mean,amp_mean], [mean_var,len_var,amp_var] = vgp.get_marginal(ZZ[None,...])\n\nmean_mean = mean_mean[0,:,0].numpy()\nmean_std = mean_var[:,0].numpy()**0.5\n\n\nlen_mean = len_mean[0,:,0].numpy()\nlen_std = len_var[:,0].numpy()**0.5\n\namp_mean = amp_mean[0,:,0].numpy()\namp_std = amp_var[:,0].numpy()**0.5\n\n\nf, (ax1, ax2,ax3) = plt.subplots(1, 3,figsize=(30,10))\n\nax3.plot(ZZ,tf.math.softplus(vgp.mean_mean + mean_mean),color='C1')\nax3.fill_between(ZZ[:,0],tf.math.softplus(vgp.mean_mean + mean_mean - 1.28*mean_std),tf.math.softplus(vgp.mean_mean + mean_mean + 1.28*mean_std),color='C1',alpha=0.25)\nax3.fill_between(ZZ[:,0],tf.math.softplus(vgp.mean_mean + mean_mean - 1.96*mean_std),tf.math.softplus(vgp.mean_mean + mean_mean + 1.96*mean_std),color='C1',alpha=0.25)\nax3.fill_between(ZZ[:,0],tf.math.softplus(vgp.mean_mean + mean_mean - 2.58*mean_std),tf.math.softplus(vgp.mean_mean + mean_mean + 2.58*mean_std),color='C1',alpha=0.25)\n\nax1.plot(ZZ,tf.math.softplus(vgp.mean_len + len_mean),color='C1')\nax1.fill_between(ZZ[:,0],tf.math.softplus(vgp.mean_len + len_mean - 1.28*len_std),tf.math.softplus(vgp.mean_len + len_mean + 1.28*len_std),color='C1',alpha=0.25)\nax1.fill_between(ZZ[:,0],tf.math.softplus(vgp.mean_len + len_mean - 1.96*len_std),tf.math.softplus(vgp.mean_len + len_mean + 1.96*len_std),color='C1',alpha=0.25)\nax1.fill_between(ZZ[:,0],tf.math.softplus(vgp.mean_len + len_mean - 2.58*len_std),tf.math.softplus(vgp.mean_len + len_mean + 2.58*len_std),color='C1',alpha=0.25)\n\nax2.plot(ZZ,tf.math.softplus(vgp.mean_amp + amp_mean),color='C1')\nax2.fill_between(ZZ[:,0],tf.math.softplus(vgp.mean_amp + amp_mean - 1.28*amp_std),tf.math.softplus(vgp.mean_amp + amp_mean + 1.28*amp_std),color='C1',alpha=0.25)\nax2.fill_between(ZZ[:,0],tf.math.softplus(vgp.mean_amp + amp_mean - 1.96*amp_std),tf.math.softplus(vgp.mean_amp + amp_mean + 1.96*amp_std),color='C1',alpha=0.25)\nax2.fill_between(ZZ[:,0],tf.math.softplus(vgp.mean_amp + amp_mean - 2.58*amp_std),tf.math.softplus(vgp.mean_amp + amp_mean + 2.58*amp_std),color='C1',alpha=0.25)\n\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2022-03-09T09:26:53.764743Z","iopub.execute_input":"2022-03-09T09:26:53.765018Z","iopub.status.idle":"2022-03-09T09:26:54.325431Z","shell.execute_reply.started":"2022-03-09T09:26:53.764987Z","shell.execute_reply":"2022-03-09T09:26:54.324796Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#!pip install pickle5","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pickle5 as pickle\n\noutputvars = []\n\nfor v in vgp.trainable_variables:\n    outputvars.append(v.numpy())\n    \n#make sure you change the names for each replicate dataset !       \nwith open('opt_power_1mil.pkl', 'wb') as f:\n    pickle.dump(outputvars, f)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"np.save('T_ind_power_1mil',inducing_index_points)    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pickle5 as pickle\n\n#Load the inducing points and the optimized parameters \nwith open('../input/powerresultsvi/power.pkl', 'rb') as f:\n    loadp = pickle.load(f)\n    \ninducing_index_points = np.load('../input/powerresultsvi/T_ind_power.npy')    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vgp = nsgpVI(kernel_len,kernel_amp,n_inducing_points=num_inducing_points_,inducing_index_points=inducing_index_points,dataset=dataset,num_training_points=num_training_points_, num_sequential_samples=20,num_parallel_samples=10,init_observation_noise_variance=0.005**2)  \n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Load the parameters !!!\nfor np_v, tf_v in zip(loadp,vgp.trainable_variables):\n    tf_v.assign(np_v)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nZZ = np.linspace(0,24,200)[:,None]\n\n[len_mean,amp_mean], [len_var,amp_var] = vgp.get_marginal(ZZ[None,...])\n\n\n# len_mean, len_var = vgp.get_len_cond(ZZ[None,...],full_cov=False)\nlen_mean = len_mean[0,:,0].numpy()\nlen_std = len_var[:,0].numpy()**0.5\n\n\n\n# amp_mean, amp_var = vgp.get_amp_cond(ZZ[None,...],full_cov=False)\namp_mean = amp_mean[0,:,0].numpy()\namp_std = amp_var[:,0].numpy()**0.5\n\n\nf, (ax1, ax2) = plt.subplots(1, 2,figsize=(16,6))\n\n\nax1.plot(ZZ,tf.math.softplus(vgp.mean_len + len_mean),color='C1')\nax1.fill_between(ZZ[:,0],tf.math.softplus(vgp.mean_len + len_mean - 1.28*len_std),tf.math.softplus(vgp.mean_len + len_mean + 1.28*len_std),color='C1',alpha=0.25)\nax1.fill_between(ZZ[:,0],tf.math.softplus(vgp.mean_len + len_mean - 1.96*len_std),tf.math.softplus(vgp.mean_len + len_mean + 1.96*len_std),color='C1',alpha=0.25)\nax1.fill_between(ZZ[:,0],tf.math.softplus(vgp.mean_len + len_mean - 2.58*len_std),tf.math.softplus(vgp.mean_len + len_mean + 2.58*len_std),color='C1',alpha=0.25)\n\nax1.set_xlabel('Time',size=15)\nax1.set_ylabel('Average power consumption persistence',size=15)\nax1.text(-0.05,1,'A', size=20, transform=ax1.transAxes)\nax1.tick_params(axis='both', which='major', labelsize=15)\ntick_locator = tick.MaxNLocator(nbins=7)\n\nax2.plot(ZZ,tf.math.softplus(vgp.mean_amp + amp_mean),color='C1')\nax2.fill_between(ZZ[:,0],tf.math.softplus(vgp.mean_amp + amp_mean - 1.28*amp_std),tf.math.softplus(vgp.mean_amp + amp_mean + 1.28*amp_std),color='C1',alpha=0.25)\nax2.fill_between(ZZ[:,0],tf.math.softplus(vgp.mean_amp + amp_mean - 1.96*amp_std),tf.math.softplus(vgp.mean_amp + amp_mean + 1.96*amp_std),color='C1',alpha=0.25)\nax2.fill_between(ZZ[:,0],tf.math.softplus(vgp.mean_amp + amp_mean - 2.58*amp_std),tf.math.softplus(vgp.mean_amp + amp_mean + 2.58*amp_std),color='C1',alpha=0.25)\n\nax2.set_xlabel('Time',size=15)\nax2.set_ylabel('Average power consumption variance',size=15)\nax2.text(-0.1,1,'B', size=20, transform=ax2.transAxes)\nax2.tick_params(axis='both', which='major', labelsize=15)\ntick_locator = tick.MaxNLocator(nbins=7)\n\nplt.savefig(\"power_inference_vi.pdf\",dpi=600)\n\nplt.show()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}