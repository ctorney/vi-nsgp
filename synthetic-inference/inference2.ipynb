{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c0eee202",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "import random\n",
    "\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "from tqdm import tqdm\n",
    "\n",
    "#import pickle5 as pickle\n",
    "\n",
    "from IPython import display\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.ticker as tick\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from nsgp_vi2 import nsgpVI\n",
    "# We'll use double precision throughout for better numerics.\n",
    "dtype = np.float64\n",
    "\n",
    "\n",
    "from addons.gradient_accumulator import GradientAccumulator\n",
    "\n",
    "tfb = tfp.bijectors\n",
    "tfd = tfp.distributions\n",
    "tfk = tfp.math.psd_kernels\n",
    "\n",
    "plt.style.use('ggplot') \n",
    "plt.style.use('seaborn-paper') \n",
    "plt.style.use('seaborn-whitegrid') \n",
    "#v3 i.e. version 3, given by the new way(third way) of calculating the batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f153c5fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../Data/ns_synthetic_data_indv_128.csv')\n",
    "T = df['Time'].values[:,None]\n",
    "ID = df['ID'].values\n",
    "X = np.array(df['observations']).reshape(len(T),1)\n",
    "true_len = np.array(df['Lengthscale']).reshape(len(T),1)\n",
    "true_var = np.array(df['Variance']).reshape(len(T),1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f45137c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#T_ = tf.convert_to_tensor(T, dtype=tf.float64)\n",
    "#X_ = tf.convert_to_tensor(X, dtype=tf.float64)\n",
    "num_training_points_ = T.shape[0]\n",
    "\n",
    "num_inducing_points_ = 50\n",
    "#inducing_index_points = np.random.uniform(0.0,60*24.,size=num_inducing_points_)[..., np.newaxis]#np.linspace(0., 60*24., num_inducing_points_, endpoint=False)[..., np.newaxis]\n",
    "inducing_index_points = np.linspace(0., 60*24., num_inducing_points_, endpoint=False)[..., np.newaxis]\n",
    "\n",
    "np.random.shuffle(inducing_index_points)\n",
    "#x_train_ = T_ # observation_index_points i.e. the time index points\n",
    "#inducing_index_points\n",
    "#y_train_ = X_ #observations i.e. data X, the original positions\n",
    "\n",
    "\n",
    "#kernel_amp = tfk.Linear(bias_variance=np.float64(0.0), slope_variance=np.float64(0.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "10ca551f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# BATCH_SIZE=2\n",
    "# SEG_LENGTH=1024\n",
    "\n",
    "# allT = []\n",
    "# allX = []\n",
    "# for i in np.unique(df['ID'].values):\n",
    "#     allT.append(df['Time'][df['ID'].values == i].values[...,None])\n",
    "#     allX.append(df['observations'][df['ID'].values == i].values[...,None])\n",
    "\n",
    "\n",
    "# def get_generator(T,X):\n",
    "#     def segment_generator():\n",
    "#         rand_start = np.random.randint(T.shape[0])\n",
    "#         max_index = T.shape[0]\n",
    "#         if rand_start+SEG_LENGTH>max_index:\n",
    "#             jj = np.arange(max_index-SEG_LENGTH,max_index)\n",
    "#         else:\n",
    "#             jj = np.arange(rand_start,rand_start+SEG_LENGTH)\n",
    "            \n",
    "        \n",
    "#         yield np.reshape(np.take(T,jj,mode='wrap',axis=0),(SEG_LENGTH,T.shape[-1])), \\\n",
    "#                 np.reshape(np.take(X,jj,mode='wrap',axis=0),(SEG_LENGTH,X.shape[-1]))\n",
    "\n",
    "#     return segment_generator\n",
    "\n",
    "# inner_dataset = [tf.data.Dataset.from_generator(get_generator(TT,XX), (tf.float64)) for TT,XX in zip(allT,allX)]\n",
    "\n",
    "\n",
    "# dataset = tf.data.experimental.sample_from_datasets(inner_dataset)\n",
    "# dataset = dataset.map(lambda dd: (dd[0],dd[1]))\n",
    "\n",
    "# dataset = dataset.shuffle(len(inner_dataset), reshuffle_each_iteration=True)\n",
    "# dataset = dataset.batch(BATCH_SIZE)\n",
    "\n",
    "# dataset = dataset.repeat(2)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d9894908",
   "metadata": {},
   "outputs": [],
   "source": [
    "#num_training_points_//len(allT)/SEG_LENGTH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fe8ba5e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE=8\n",
    "SEG_LENGTH=1024\n",
    "\n",
    "allT = []\n",
    "allX = []\n",
    "for i in np.unique(df['ID'].values):\n",
    "    allT.append(df['Time'][df['ID'].values == i].values[...,None])\n",
    "    allX.append(df['observations'][df['ID'].values == i].values[...,None])\n",
    "\n",
    "class segment_generator:\n",
    "    def __iter__(self):\n",
    "        self.i = 0\n",
    "        self.max_i = len(allT)\n",
    "        \n",
    "        self.i_list = np.arange(0,self.max_i)\n",
    "        np.random.shuffle(self.i_list)\n",
    "        \n",
    "        self.samples_per_epoch = num_training_points_//(self.max_i*SEG_LENGTH)\n",
    "        \n",
    "        self.loop_count = 0\n",
    "        \n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        \n",
    "        if self.i == self.max_i:\n",
    "            self.i = 0\n",
    "            self.loop_count+=1\n",
    "            self.i_list = np.arange(0,self.max_i)\n",
    "            np.random.shuffle(self.i_list)\n",
    "            if self.samples_per_epoch==self.loop_count:\n",
    "                raise StopIteration\n",
    "            \n",
    "        T = allT[self.i_list[self.i]]\n",
    "        X = allX[self.i_list[self.i]]\n",
    "\n",
    "        index = np.random.randint(T.shape[0])\n",
    "        max_index = T.shape[0]\n",
    "        index = min(index,max_index-SEG_LENGTH)\n",
    "\n",
    "\n",
    "        TT = T[index:index+SEG_LENGTH]\n",
    "        XX = X[index:index+SEG_LENGTH]\n",
    "    \n",
    "        self.i += 1\n",
    "\n",
    "        return TT,XX\n",
    "\n",
    "        \n",
    "dataset = tf.data.Dataset.from_generator(segment_generator, (tf.float64)) \n",
    "dataset = dataset.map(lambda dd: (dd[0],dd[1]))\n",
    "dataset = dataset.batch(BATCH_SIZE)\n",
    "d_size = T.shape[0]//SEG_LENGTH//(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4885a989",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0cf366e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# for d in dataset:\n",
    "#     print(d[0].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "455fa1d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel_len_a = tfp.util.TransformedVariable(0.5, tfb.Softplus(),dtype=tf.float64, name='k_len_a',trainable=False)\n",
    "kernel_len_l = tfp.util.TransformedVariable(60.0,tfb.Softplus(),dtype=tf.float64, name='k_len_l',trainable=False)\n",
    "\n",
    "# amplitude kernel parameters, lower levels\n",
    "kernel_amp_a = tfp.util.TransformedVariable(1.0, tfb.Softplus(), dtype=tf.float64, name='k_amp_a',trainable=False)\n",
    "kernel_amp_l = tfp.util.TransformedVariable(60.0,tfb.Softplus(), dtype=tf.float64, name='k_amp_l',trainable=False)\n",
    "\n",
    "    #kernels on the second layer\n",
    "kernel_len = tfk.ExponentiatedQuadratic(kernel_len_a,kernel_len_l)\n",
    "kernel_amp = tfk.ExponentiatedQuadratic(kernel_amp_a,kernel_amp_l)\n",
    "\n",
    "vgp = nsgpVI(kernel_len,kernel_amp,n_inducing_points=num_inducing_points_,inducing_index_points=inducing_index_points,dataset=dataset,num_training_points=num_training_points_, num_sequential_samples=1,num_parallel_samples=10,init_observation_noise_variance=0.005**2)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d79678ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/200 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /anaconda/envs/py37_tensorflow/lib/python3.7/site-packages/tensorflow/python/ops/linalg/linear_operator_lower_triangular.py:167: calling LinearOperator.__init__ (from tensorflow.python.ops.linalg.linear_operator) with graph_parents is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Do not pass `graph_parents`.  They will  no longer be used.\n"
     ]
    }
   ],
   "source": [
    "loss = vgp.optimize(BATCH_SIZE, SEG_LENGTH, NUM_EPOCHS=200, WARM_UP=5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e0d5c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e3accb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ZZ = np.linspace(0,24*60,200)[:,None]\n",
    "\n",
    "len_mean, len_var = vgp.get_len_cond(ZZ[None,...],full_cov=False)\n",
    "len_mean = len_mean[0,:,0].numpy()\n",
    "len_std = len_var[:,0].numpy()**0.5\n",
    "\n",
    "\n",
    "\n",
    "amp_mean, amp_var = vgp.get_amp_cond(ZZ[None,...],full_cov=False)\n",
    "amp_mean = amp_mean[0,:,0].numpy()\n",
    "amp_std = amp_var[:,0].numpy()**0.5\n",
    "\n",
    "\n",
    "# for i in range(S):\n",
    "#     plt.plot(ZZ,len_vals[i],color='C0',alpha=0.01)\n",
    "# #plt.plot(T,true_len,'.',markersize=1,color='blue')\n",
    "# plt.plot(vgp.len_inducing_index_points.numpy(), np.ones_like(vgp.len_inducing_index_points.numpy()),'o')\n",
    "\n",
    "\n",
    "f, (ax1, ax2) = plt.subplots(1, 2,figsize=(16,6))\n",
    "\n",
    "\n",
    "ax1.plot(ZZ,tf.math.softplus(vgp.mean_len + len_mean),color='C1')\n",
    "\n",
    "ax1.fill_between(ZZ[:,0],tf.math.softplus(vgp.mean_len + len_mean - 1.96*len_std),tf.math.softplus(vgp.mean_len + len_mean + 1.96*len_std),color='C1',alpha=0.5)\n",
    "#ax1.plot(T,true_len,'.',markersize=1,color='red')\n",
    "ax1.plot(T[:8192],true_len[:8192],'--',markersize=1,color='k',alpha=0.5)\n",
    "\n",
    "\n",
    "ax2.plot(T[:8192],true_var[:8192],'--',markersize=1,color='k',alpha=0.5)\n",
    "ax2.plot(ZZ,tf.math.softplus(vgp.mean_amp + amp_mean),color='C1')\n",
    "\n",
    "ax2.fill_between(ZZ[:,0],tf.math.softplus(vgp.mean_amp + amp_mean - 1.96*amp_std),tf.math.softplus(vgp.mean_amp + amp_mean + 1.96*amp_std),color='C1',alpha=0.5)\n",
    "\n",
    "plt.show()\n",
    "# plt.plot(ZZ,tf.math.softplus(vgp.mean_amp + amp_mean[0]+amp_var[:,0]**0.5),co"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc6a19a",
   "metadata": {},
   "outputs": [],
   "source": [
    "vgp.trainable_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beba5020",
   "metadata": {},
   "outputs": [],
   "source": [
    "vgp.len_variational_inducing_observations_scale.variables[0]._trainable=False\n",
    "vgp.amp_variational_inducing_observations_scale.variables[0]._trainable=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec6cc93e",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS=1000 # 8 training\n",
    "\n",
    "#NUM_EPOCHS=500 # 128 training\n",
    "\n",
    "initial_learning_rate = float(BATCH_SIZE*SEG_LENGTH)/float(num_training_points_)\n",
    "\n",
    "steps_per_epoch = num_training_points_//(BATCH_SIZE*SEG_LENGTH)\n",
    "learning_rate = tf.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate=initial_learning_rate,\n",
    "    decay_steps=steps_per_epoch,\n",
    "    decay_rate=0.95,\n",
    "    staircase=True)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "optimizer = GradientAccumulator(optimizer, accum_steps=10, reduction='MEAN')\n",
    "\n",
    "\n",
    "#@tf.function\n",
    "def optimize(x_train_batch, y_train_batch):\n",
    "    \n",
    "    kl_weight = tf.reduce_sum(tf.ones_like(x_train_batch))/num_training_points_\n",
    "\n",
    "    with tf.GradientTape(watch_accessed_variables=True) as tape:\n",
    "        # Create the loss function we want to optimize.\n",
    "        loss = vgp.variational_loss(observations=y_train_batch,observation_index_points=x_train_batch,kl_weight=kl_weight)\n",
    "    grads = tape.gradient(loss, vgp.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, vgp.trainable_variables))\n",
    "    return loss\n",
    "\n",
    "#pbar = tqdm(range(NUM_EPOCHS))\n",
    "loss_history = np.zeros((NUM_EPOCHS))\n",
    "\n",
    "for i in range(NUM_EPOCHS):\n",
    "    \n",
    "    lnp = 0.0\n",
    "    for batch in dataset:\n",
    "    #batch = next(iter(dataset))\n",
    "        for s in range(10):\n",
    "\n",
    "            loss = optimize(*batch)\n",
    "        lnp += loss.numpy()\n",
    "    \n",
    "    vgp.len_variational_inducing_observations_scale.variables[0]._trainable=True\n",
    "    vgp.amp_variational_inducing_observations_scale.variables[0]._trainable=True\n",
    "    try:\n",
    "        ZZ = np.linspace(0,24*60,200)[:,None]\n",
    "\n",
    "        len_mean, len_var = vgp.get_len_cond(ZZ[None,...],full_cov=False)\n",
    "        len_mean = len_mean[0,:,0].numpy()\n",
    "        len_std = len_var[:,0].numpy()**0.5\n",
    "\n",
    "\n",
    "\n",
    "        amp_mean, amp_var = vgp.get_amp_cond(ZZ[None,...],full_cov=False)\n",
    "        amp_mean = amp_mean[0,:,0].numpy()\n",
    "        amp_std = amp_var[:,0].numpy()**0.5\n",
    "\n",
    "\n",
    "        # for i in range(S):\n",
    "        #     plt.plot(ZZ,len_vals[i],color='C0',alpha=0.01)\n",
    "        # #plt.plot(T,true_len,'.',markersize=1,color='blue')\n",
    "        # plt.plot(vgp.len_inducing_index_points.numpy(), np.ones_like(vgp.len_inducing_index_points.numpy()),'o')\n",
    "\n",
    "\n",
    "        # plt.show()\n",
    "        plt.close()\n",
    "        plt.clf()\n",
    "        f, (ax1, ax2) = plt.subplots(1, 2,figsize=(30,12))\n",
    "\n",
    "       \n",
    "        ax1.plot(ZZ,tf.math.softplus(vgp.mean_len + len_mean),color='C1')\n",
    "\n",
    "        ax1.fill_between(ZZ[:,0],tf.math.softplus(vgp.mean_len + len_mean - 1.96*len_std),tf.math.softplus(vgp.mean_len + len_mean + 1.96*len_std),color='C1',alpha=0.5)\n",
    "        ax1.plot(T,true_len,'.',markersize=1,color='red')\n",
    "        \n",
    "        \n",
    "        ax2.plot(ZZ,tf.math.softplus(vgp.mean_amp + amp_mean),color='C1')\n",
    "\n",
    "        ax2.fill_between(ZZ[:,0],tf.math.softplus(vgp.mean_amp + amp_mean - 1.96*amp_std),tf.math.softplus(vgp.mean_amp + amp_mean + 1.96*amp_std),color='C1',alpha=0.5)\n",
    "        ax2.plot(T,true_var,'.',markersize=1,color='red')\n",
    "        display.display(plt.gcf())\n",
    "        display.clear_output(wait=True)\n",
    "        #time.sleep(1)\n",
    "    except KeyboardInterrupt:\n",
    "        break\n",
    "\n",
    "    #pbar.set_description(\"Loss %f\" % lnp)\n",
    "\n",
    "    loss_history[i] = lnp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f45665",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "S=1000\n",
    "ZZ = np.linspace(0,24*60,200)[:,None]\n",
    "ZZ.shape\n",
    "len_vals = tf.squeeze(vgp.get_len_samples(ZZ[None,...],S))\n",
    "amp_vals = tf.squeeze(vgp.get_amp_samples(ZZ[None,...],S))\n",
    "\n",
    "\n",
    "for i in range(S):\n",
    "    plt.plot(ZZ,len_vals[i],color='C0',alpha=0.01)\n",
    "plt.plot(T,true_len,'.',markersize=1,color='blue')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "for i in range(S):\n",
    "    plt.plot(ZZ,amp_vals[i],color='C0',alpha=0.01)\n",
    "plt.plot(T,true_var,'.',markersize=1,color='blue')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b458b83d",
   "metadata": {},
   "outputs": [],
   "source": [
    "len_mean, len_var = vgp.get_len_cond(ZZ[None,...],full_cov=False)\n",
    "len_mean = len_mean[0,:,0].numpy()\n",
    "len_std = len_var[:,0].numpy()**0.5\n",
    "\n",
    "\n",
    "\n",
    "amp_mean, amp_var = vgp.get_amp_cond(ZZ[None,...],full_cov=False)\n",
    "amp_mean = amp_mean[0,:,0].numpy()\n",
    "amp_std = amp_var[:,0].numpy()**0.5\n",
    "\n",
    "\n",
    "# for i in range(S):\n",
    "#     plt.plot(ZZ,len_vals[i],color='C0',alpha=0.01)\n",
    "# #plt.plot(T,true_len,'.',markersize=1,color='blue')\n",
    "# plt.plot(vgp.len_inducing_index_points.numpy(), np.ones_like(vgp.len_inducing_index_points.numpy()),'o')\n",
    "\n",
    "\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "plt.plot(ZZ,tf.math.softplus(vgp.mean_len + len_mean),color='C1')\n",
    "\n",
    "plt.fill_between(ZZ[:,0],tf.math.softplus(vgp.mean_len + len_mean - 1.96*len_std),tf.math.softplus(vgp.mean_len + len_mean + 1.96*len_std),color='C1',alpha=0.5)\n",
    "plt.plot(T,true_len,'.',markersize=1,color='red')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.plot(ZZ,tf.math.softplus(vgp.mean_amp + amp_mean),color='C1')\n",
    "\n",
    "plt.fill_between(ZZ[:,0],tf.math.softplus(vgp.mean_amp + amp_mean - 1.96*amp_std),tf.math.softplus(vgp.mean_amp + amp_mean + 1.96*amp_std),color='C1',alpha=0.5)\n",
    "plt.plot(T,true_var,'.',markersize=1,color='red')\n",
    "#plt.ylim(0,1)\n",
    "plt.show()\n",
    "# plt.plot(ZZ,tf.math.softplus(vgp.mean_amp + amp_mean[0]+amp_var[:,0]**0.5),co"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f8c9b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "vgp.trainable_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42bb1bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "vgp.len_variational_inducing_observations_scale#.Trainable=True\n",
    "vgp.amp_variational_inducing_observations_scale.variables[0]._trainable=True# pretransformed_input.trainable=True# Trainable # .Trainable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c504d3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
